{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spark \n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkContext is the entry point for spark RDD. \n",
    "# SparkSession is the new entry point for all context, including sql context, hive context etc. \n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "### Part 1\n",
    "Now let's calculate covariance and correlation by ourselves using ApacheSpark\n",
    "\n",
    "1st we crate two random RDDâ€™s, which shouldn't correlate at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "rddX = sc.parallelize(random.sample(list(range(100)),100))\n",
    "rddY = sc.parallelize(random.sample(list(range(100)),100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the mean, note that we explicitly cast the denominator to float in order to obtain a float instead of int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.5 49.5\n"
     ]
    }
   ],
   "source": [
    "meanX = rddX.sum()/rddX.count()\n",
    "meanY = rddY.sum()/rddY.count()\n",
    "print(meanX, meanY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(92, 66),\n",
       " (4, 40),\n",
       " (85, 28),\n",
       " (44, 83),\n",
       " (84, 9),\n",
       " (51, 90),\n",
       " (29, 94),\n",
       " (32, 53),\n",
       " (17, 67),\n",
       " (33, 46)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddXY = rddX.zip(rddY)\n",
    "rddXY.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-82.04"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covXY = rddXY.map(lambda x_y: (x_y[0]-meanX)*(x_y[1]-meanY)).sum()/rddXY.count()\n",
    "covXY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance is not a normalized measure. Therefore we use it to calculate correlation. But before that we need to calculate the indivicual standard deviations first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = rddXY.count()\n",
    "sdX = sqrt(rddX.map(lambda x: pow(x-meanX,2)).sum()/n)\n",
    "sdY = sqrt(rddY.map(lambda y: pow(y-meanY,2)).sum()/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.86607004772212 28.86607004772212\n"
     ]
    }
   ],
   "source": [
    "print(sdX, sdY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrXY = covXY/(sdX*sdY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.09845784578457846"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrXY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Now we want to create a correlation matrix out of the four RDDS used in the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "column1 = sc.parallelize(range(100))\n",
    "column2 = sc.parallelize(range(100,200))\n",
    "column3 = sc.parallelize(list(reversed(range(100))))\n",
    "column4 = sc.parallelize(random.sample(range(100),100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(((0, 100), 99), 85),\n",
       " (((1, 101), 98), 3),\n",
       " (((2, 102), 97), 77),\n",
       " (((3, 103), 96), 65),\n",
       " (((4, 104), 95), 22),\n",
       " (((5, 105), 94), 48),\n",
       " (((6, 106), 93), 97),\n",
       " (((7, 107), 92), 71),\n",
       " (((8, 108), 91), 94),\n",
       " (((9, 109), 90), 26)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column1.zip(column2).zip(column3).zip(column4).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = column1.zip(column2).zip(column3).zip(column4)\\\n",
    ".map(lambda a_b_c_d : (a_b_c_d[0][0][0],a_b_c_d[0][0][1],a_b_c_d[0][1],a_b_c_d[1]) )\\\n",
    ".map(lambda a_b_c_d : [a_b_c_d[0],a_b_c_d[1],a_b_c_d[2],a_b_c_d[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 100, 99, 85],\n",
       " [1, 101, 98, 3],\n",
       " [2, 102, 97, 77],\n",
       " [3, 103, 96, 65],\n",
       " [4, 104, 95, 22],\n",
       " [5, 105, 94, 48],\n",
       " [6, 106, 93, 97],\n",
       " [7, 107, 92, 71],\n",
       " [8, 108, 91, 94],\n",
       " [9, 109, 90, 26]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        , -1.        , -0.03450345],\n",
       "       [ 1.        ,  1.        , -1.        , -0.03450345],\n",
       "       [-1.        , -1.        ,  1.        ,  0.03450345],\n",
       "       [-0.03450345, -0.03450345,  0.03450345,  1.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
